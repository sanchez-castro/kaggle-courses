{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing, metrics\nimport lightgbm as lgb\n\n# Set up code checking\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.feature_engineering.ex4 import *\n\nimport os\n\nclicks = pd.read_parquet('../input/feature-engineering-data/baseline_data.pqt')\ndata_files = ['count_encodings.pqt',\n              'catboost_encodings.pqt',\n              'interactions.pqt',\n              'past_6hr_events.pqt',\n              'downloads.pqt',\n              'time_deltas.pqt',\n              'svd_encodings.pqt']\ndata_root = '../input/feature-engineering-data'\nfor file in data_files:\n    features = pd.read_parquet(os.path.join(data_root, file))\n    clicks = clicks.join(features)\n\ndef get_data_splits(dataframe, valid_fraction=0.1):\n\n    dataframe = dataframe.sort_values('click_time')\n    valid_rows = int(len(dataframe) * valid_fraction)\n    train = dataframe[:-valid_rows * 2]\n    # valid size == test size, last two sections of the data\n    valid = dataframe[-valid_rows * 2:-valid_rows]\n    test = dataframe[-valid_rows:]\n    \n    return train, valid, test\n\ndef train_model(train, valid, test=None, feature_cols=None):\n    if feature_cols is None:\n        feature_cols = train.columns.drop(['click_time', 'attributed_time',\n                                           'is_attributed'])\n    dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n    dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n    \n    param = {'num_leaves': 64, 'objective': 'binary', \n             'metric': 'auc', 'seed': 7}\n    num_round = 1000\n    print(\"Training model!\")\n    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n                    early_stopping_rounds=20, verbose_eval=False)\n    \n    valid_pred = bst.predict(valid[feature_cols])\n    valid_score = metrics.roc_auc_score(valid['is_attributed'], valid_pred)\n    print(f\"Validation AUC score: {valid_score}\")\n    \n    if test is not None: \n        test_pred = bst.predict(test[feature_cols])\n        test_score = metrics.roc_auc_score(test['is_attributed'], test_pred)\n        return bst, valid_score, test_score\n    else:\n        return bst, valid_score","execution_count":1,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/pyarrow/pandas_compat.py:707: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n  labels = getattr(columns, 'labels', None) or [\n/opt/conda/lib/python3.6/site-packages/pyarrow/pandas_compat.py:734: FutureWarning: the 'labels' keyword is deprecated, use 'codes' instead\n  return pd.MultiIndex(levels=new_levels, labels=labels, names=columns.names)\n/opt/conda/lib/python3.6/site-packages/pyarrow/pandas_compat.py:751: FutureWarning: .labels was deprecated in version 0.24.0. Use .codes instead.\n  labels, = index.labels\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"## Baseline Score"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, valid, test = get_data_splits(clicks)\n_, baseline_score, _ = train_model(train, valid, test)","execution_count":2,"outputs":[{"output_type":"stream","text":"Training model!\nValidation AUC score: 0.9658334271834417\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 1) Which data to use for feature selection?\n\nSince many feature selection methods require calculating statistics from the dataset, should you use all the data for feature selection?\n\nRun the following line after you've decided your answer."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check your answer (Run this code cell to receive credit!)\nq_1.solution()","execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"learnTutorialId\": 273, \"questionId\": \"1_FeatureSelectionData\", \"learnToolsVersion\": \"0.3.2\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: Including validation and test data within the feature selection is a source of leakage. You'll want to perform feature selection on the train set only, then use the results there to remove features from the validation and test sets.","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> Including validation and test data within the feature selection is a source of leakage. You'll want to perform feature selection on the train set only, then use the results there to remove features from the validation and test sets."},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train.columns)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"94"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"       ip  app  device  os  channel          click_time      attributed_time  \\\n0   27226    3       1  13      120 2017-11-06 15:13:23                 None   \n1  110007   35       1  13       10 2017-11-06 15:41:07  2017-11-07 08:17:19   \n2    1047    6       1  13      157 2017-11-06 15:42:32                 None   \n3   76270    3       1  13      120 2017-11-06 15:56:17                 None   \n4   57862    3       1  13      120 2017-11-06 15:57:01                 None   \n\n   is_attributed  day  hour  ...  device_channel_svd_0  device_channel_svd_1  \\\n0              0    6    15  ...              0.998937             -0.026614   \n1              1    6    15  ...              0.998937             -0.026614   \n2              0    6    15  ...              0.998937             -0.026614   \n3              0    6    15  ...              0.998937             -0.026614   \n4              0    6    15  ...              0.998937             -0.026614   \n\n   device_channel_svd_2  device_channel_svd_3  device_channel_svd_4  \\\n0              0.033651             -0.016794              0.001659   \n1              0.033651             -0.016794              0.001659   \n2              0.033651             -0.016794              0.001659   \n3              0.033651             -0.016794              0.001659   \n4              0.033651             -0.016794              0.001659   \n\n   os_channel_svd_0  os_channel_svd_1  os_channel_svd_2  os_channel_svd_3  \\\n0          0.632548          -0.05079         -0.045754          0.086897   \n1          0.632548          -0.05079         -0.045754          0.086897   \n2          0.632548          -0.05079         -0.045754          0.086897   \n3          0.632548          -0.05079         -0.045754          0.086897   \n4          0.632548          -0.05079         -0.045754          0.086897   \n\n   os_channel_svd_4  \n0           -0.3227  \n1           -0.3227  \n2           -0.3227  \n3           -0.3227  \n4           -0.3227  \n\n[5 rows x 94 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ip</th>\n      <th>app</th>\n      <th>device</th>\n      <th>os</th>\n      <th>channel</th>\n      <th>click_time</th>\n      <th>attributed_time</th>\n      <th>is_attributed</th>\n      <th>day</th>\n      <th>hour</th>\n      <th>...</th>\n      <th>device_channel_svd_0</th>\n      <th>device_channel_svd_1</th>\n      <th>device_channel_svd_2</th>\n      <th>device_channel_svd_3</th>\n      <th>device_channel_svd_4</th>\n      <th>os_channel_svd_0</th>\n      <th>os_channel_svd_1</th>\n      <th>os_channel_svd_2</th>\n      <th>os_channel_svd_3</th>\n      <th>os_channel_svd_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>27226</td>\n      <td>3</td>\n      <td>1</td>\n      <td>13</td>\n      <td>120</td>\n      <td>2017-11-06 15:13:23</td>\n      <td>None</td>\n      <td>0</td>\n      <td>6</td>\n      <td>15</td>\n      <td>...</td>\n      <td>0.998937</td>\n      <td>-0.026614</td>\n      <td>0.033651</td>\n      <td>-0.016794</td>\n      <td>0.001659</td>\n      <td>0.632548</td>\n      <td>-0.05079</td>\n      <td>-0.045754</td>\n      <td>0.086897</td>\n      <td>-0.3227</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>110007</td>\n      <td>35</td>\n      <td>1</td>\n      <td>13</td>\n      <td>10</td>\n      <td>2017-11-06 15:41:07</td>\n      <td>2017-11-07 08:17:19</td>\n      <td>1</td>\n      <td>6</td>\n      <td>15</td>\n      <td>...</td>\n      <td>0.998937</td>\n      <td>-0.026614</td>\n      <td>0.033651</td>\n      <td>-0.016794</td>\n      <td>0.001659</td>\n      <td>0.632548</td>\n      <td>-0.05079</td>\n      <td>-0.045754</td>\n      <td>0.086897</td>\n      <td>-0.3227</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1047</td>\n      <td>6</td>\n      <td>1</td>\n      <td>13</td>\n      <td>157</td>\n      <td>2017-11-06 15:42:32</td>\n      <td>None</td>\n      <td>0</td>\n      <td>6</td>\n      <td>15</td>\n      <td>...</td>\n      <td>0.998937</td>\n      <td>-0.026614</td>\n      <td>0.033651</td>\n      <td>-0.016794</td>\n      <td>0.001659</td>\n      <td>0.632548</td>\n      <td>-0.05079</td>\n      <td>-0.045754</td>\n      <td>0.086897</td>\n      <td>-0.3227</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>76270</td>\n      <td>3</td>\n      <td>1</td>\n      <td>13</td>\n      <td>120</td>\n      <td>2017-11-06 15:56:17</td>\n      <td>None</td>\n      <td>0</td>\n      <td>6</td>\n      <td>15</td>\n      <td>...</td>\n      <td>0.998937</td>\n      <td>-0.026614</td>\n      <td>0.033651</td>\n      <td>-0.016794</td>\n      <td>0.001659</td>\n      <td>0.632548</td>\n      <td>-0.05079</td>\n      <td>-0.045754</td>\n      <td>0.086897</td>\n      <td>-0.3227</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>57862</td>\n      <td>3</td>\n      <td>1</td>\n      <td>13</td>\n      <td>120</td>\n      <td>2017-11-06 15:57:01</td>\n      <td>None</td>\n      <td>0</td>\n      <td>6</td>\n      <td>15</td>\n      <td>...</td>\n      <td>0.998937</td>\n      <td>-0.026614</td>\n      <td>0.033651</td>\n      <td>-0.016794</td>\n      <td>0.001659</td>\n      <td>0.632548</td>\n      <td>-0.05079</td>\n      <td>-0.045754</td>\n      <td>0.086897</td>\n      <td>-0.3227</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 94 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now we have 91 features we're using for predictions. With all these features, there is a good chance the model is overfitting the data. We might be able to reduce the overfitting by removing some features. Of course, the model's performance might decrease. But at least we'd be making the model smaller and faster without losing much performance."},{"metadata":{},"cell_type":"markdown","source":"### 2) Univariate Feature Selection\n\nBelow, use `SelectKBest` with the `f_classif` scoring function to choose 40 features from the 91 features in the data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\nfeature_cols = clicks.columns.drop(['click_time', 'attributed_time', 'is_attributed'])\ntrain, valid, test = get_data_splits(clicks)\n\n# Create the selector, keeping 40 features\nselector = SelectKBest(f_classif, k=40)\n\n# Use the selector to retrieve the best features\nX_new = selector.fit_transform(train[feature_cols], train['is_attributed']) \n\n# Get back the kept features as a DataFrame with dropped columns as all 0s\nselected_features = pd.DataFrame(selector.inverse_transform(X_new), index=train.index, columns=feature_cols) \n\n# Find the columns that were dropped\ndropped_columns = selected_features.columns[selected_features.var() == 0]\n\n# Check your answer\nq_2.check()","execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"learnTutorialId\": 273, \"questionId\": \"2_UnivariateSelection\", \"learnToolsVersion\": \"0.3.2\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Uncomment these lines if you need some guidance\n# q_2.hint()\n q_2.solution()","execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 2, \"learnTutorialId\": 273, \"questionId\": \"2_UnivariateSelection\", \"learnToolsVersion\": \"0.3.2\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: \n```python\n\n    # Do feature extraction on the training data only!\n    selector = SelectKBest(f_classif, k=40)\n    X_new = selector.fit_transform(train[feature_cols], train['is_attributed'])\n\n    # Get back the features we've kept, zero out all other features\n    selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                    index=train.index, \n                                    columns=feature_cols)\n\n    # Dropped columns have values of all 0s, so var is 0, drop them\n    dropped_columns = selected_features.columns[selected_features.var() == 0]\n```","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> \n```python\n\n    # Do feature extraction on the training data only!\n    selector = SelectKBest(f_classif, k=40)\n    X_new = selector.fit_transform(train[feature_cols], train['is_attributed'])\n\n    # Get back the features we've kept, zero out all other features\n    selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n                                    index=train.index, \n                                    columns=feature_cols)\n\n    # Dropped columns have values of all 0s, so var is 0, drop them\n    dropped_columns = selected_features.columns[selected_features.var() == 0]\n```"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = train_model(train.drop(dropped_columns, axis=1), \n                valid.drop(dropped_columns, axis=1),\n                test.drop(dropped_columns, axis=1))","execution_count":8,"outputs":[{"output_type":"stream","text":"Training model!\nValidation AUC score: 0.9625481759576047\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### 3) The best value of K\n\nWith this method we can choose the best K features, but we still have to choose K ourselves. How would you find the \"best\" value of K? That is, you want it to be small so you're keeping the best features, but not so small that it's degrading the model's performance.\n\nRun the following line after you've decided your answer."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check your answer (Run this code cell to receive credit!)\nq_3.solution()","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"learnTutorialId\": 273, \"questionId\": \"3_BestKValue\", \"learnToolsVersion\": \"0.3.2\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: To find the best value of K, you can fit multiple models with increasing values of K, then choose the smallest K with validation score above some threshold or some other criteria. A good way to do this is loop over values of K and record the validation scores for each iteration.","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> To find the best value of K, you can fit multiple models with increasing values of K, then choose the smallest K with validation score above some threshold or some other criteria. A good way to do this is loop over values of K and record the validation scores for each iteration."},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### 4) Use L1 regularization for feature selection\n\nNow try a more powerful approach using L1 regularization. Implement a function `select_features_l1` that returns a list of features to keep.\n\nUse a `LogisticRegression` classifier model with an L1 penalty to select the features. For the model, set the random state to 7 and the regularization parameter to 0.1. Fit the model then use `SelectFromModel` to return a model with the selected features.\n\nThe checking code will run your function on a sample from the dataset to provide more immediate feedback."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\n\ndef select_features_l1(X, y):\n    \"\"\" Return selected features using logistic regression with an L1 penalty \"\"\"\n    logistic = LogisticRegression(C=.1, penalty=\"l1\", random_state=7, solver='liblinear').fit(X, y)\n    model = SelectFromModel(logistic, prefit=True)\n    X_new = model.transform(X)\n    # Get back the kept features as a DataFrame with dropped columns as all 0s\n    selected_features = pd.DataFrame(model.inverse_transform(X_new), \n                                 index=X.index,\n                                 columns=X.columns)\n\n    # Dropped columns have values of all 0s, keep other columns \n    selected_columns = selected_features.columns[selected_features.var() != 0]\n    \n    return selected_columns\n\n# Check your answer\nq_4.check()","execution_count":11,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.5, \"interactionType\": 1, \"questionType\": 2, \"learnTutorialId\": 273, \"questionId\": \"4_L1Regularization\", \"learnToolsVersion\": \"0.3.2\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Correct","text/markdown":"<span style=\"color:#33cc33\">Correct</span>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Uncomment these if you're feeling stuck\nq_4.hint()\nq_4.solution()","execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 2, \"questionType\": 2, \"learnTutorialId\": 273, \"questionId\": \"4_L1Regularization\", \"learnToolsVersion\": \"0.3.2\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Hint: First fit the logistic regression model, then pass it to `SelectFromModel`. That should give you a model with the selected features, you can get the selected features with `X_new = model.transform(X)`. However, this leaves off the column labels so you'll need to get them back. The easiest way to do this is to use `model.inverse_transform(X_new)` to get back the original `X` array with the dropped columns as all zeros. Then you can create a new DataFrame with the index and columns of `X`. From there, keep the columns that aren't all zeros.","text/markdown":"<span style=\"color:#3366cc\">Hint:</span> First fit the logistic regression model, then pass it to `SelectFromModel`. That should give you a model with the selected features, you can get the selected features with `X_new = model.transform(X)`. However, this leaves off the column labels so you'll need to get them back. The easiest way to do this is to use `model.inverse_transform(X_new)` to get back the original `X` array with the dropped columns as all zeros. Then you can create a new DataFrame with the index and columns of `X`. From there, keep the columns that aren't all zeros."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 2, \"learnTutorialId\": 273, \"questionId\": \"4_L1Regularization\", \"learnToolsVersion\": \"0.3.2\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: \n```python\n\n    def select_features_l1(X, y):\n        logistic = LogisticRegression(C=0.1, penalty=\"l1\", random_state=7).fit(X, y)\n        model = SelectFromModel(logistic, prefit=True)\n\n        X_new = model.transform(X)\n        \n        # Get back the kept features as a DataFrame with dropped columns as all 0s\n        selected_features = pd.DataFrame(model.inverse_transform(X_new), \n                                        index=X.index,\n                                        columns=X.columns)\n        \n        # Dropped columns have values of all 0s, keep other columns \n        cols_to_keep = selected_features.columns[selected_features.var() != 0]\n        \n        return cols_to_keep\n```","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> \n```python\n\n    def select_features_l1(X, y):\n        logistic = LogisticRegression(C=0.1, penalty=\"l1\", random_state=7).fit(X, y)\n        model = SelectFromModel(logistic, prefit=True)\n\n        X_new = model.transform(X)\n        \n        # Get back the kept features as a DataFrame with dropped columns as all 0s\n        selected_features = pd.DataFrame(model.inverse_transform(X_new), \n                                        index=X.index,\n                                        columns=X.columns)\n        \n        # Dropped columns have values of all 0s, keep other columns \n        cols_to_keep = selected_features.columns[selected_features.var() != 0]\n        \n        return cols_to_keep\n```"},"metadata":{}}]},{"metadata":{"trusted":false},"cell_type":"code","source":"dropped_columns = feature_cols.drop(selected)\n_ = train_model(train.drop(dropped_columns, axis=1), \n                valid.drop(dropped_columns, axis=1),\n                test.drop(dropped_columns, axis=1))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 5) Feature Selection with Trees\n\nSince we're using a tree-based model, using another tree-based model for feature selection might produce better results. What would you do different to select the features using a trees classifier?\n\nRun the following line after you've decided your answer."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check your answer (Run this code cell to receive credit!)\nq_5.solution()","execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"learnTutorialId\": 273, \"questionId\": \"5_FeatureSelectionTrees\", \"learnToolsVersion\": \"0.3.2\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: You could use something like `RandomForestClassifier` or `ExtraTreesClassifier` to find feature importances. `SelectFromModel` can use the feature importances to find the best features.","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> You could use something like `RandomForestClassifier` or `ExtraTreesClassifier` to find feature importances. `SelectFromModel` can use the feature importances to find the best features."},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### 6) Top K features with L1 regularization\n\nHere you've set the regularization parameter `C=0.1` which led to some number of features being dropped. However, by setting `C` you aren't able to choose a certain number of features to keep. What would you do to keep the top K important features using L1 regularization?\n\nRun the following line after you've decided your answer."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check your answer (Run this code cell to receive credit!)\nq_6.solution()","execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"learnTutorialId\": 273, \"questionId\": \"6_L1SelectionTopK\", \"learnToolsVersion\": \"0.3.2\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Solution: To select a certain number of features with L1 regularization, you need to find the regularization parameter that leaves the desired number of features. To do this you can iterate over models with different regularization parameters from low to high and choose the one that leaves K features. Note that for the scikit-learn models C is the *inverse* of the regularization strength.","text/markdown":"<span style=\"color:#33cc99\">Solution:</span> To select a certain number of features with L1 regularization, you need to find the regularization parameter that leaves the desired number of features. To do this you can iterate over models with different regularization parameters from low to high and choose the one that leaves K features. Note that for the scikit-learn models C is the *inverse* of the regularization strength."},"metadata":{}}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":4}