{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you'll have hundreds or thousands of features after various encodings and feature generation. This can lead to two problems. First, the more features you have, the more likely you are to overfit to the training and validation sets. This will cause your model to perform worse at generalizing to new data.\n",
    "\n",
    "Secondly, the more features you have, the longer it will take to train your model and optimize hyperparameters. Also, when building user-facing products, you'll want to make inference as fast as possible. Using fewer features can speed up inference at the cost of predictive performance.\n",
    "\n",
    "To help with these issues, you'll want to use feature selection techniques to keep the most informative features for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "\n",
    "ks = pd.read_csv('./data/ks-projects-201801.csv', parse_dates=['deadline', 'launched'])\n",
    "\n",
    "\n",
    "# Drop live projects\n",
    "ks = ks.query('state != \"live\"')\n",
    "\n",
    "# Add outcome column, \"successful\" == 1, others are 0\n",
    "ks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))\n",
    "\n",
    "# Timestamp features\n",
    "ks = ks.assign(hour=ks.launched.dt.hour,\n",
    "               day=ks.launched.dt.day,\n",
    "               month=ks.launched.dt.month,\n",
    "               year=ks.launched.dt.year)\n",
    "\n",
    "# Label encoding\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "encoder = LabelEncoder()\n",
    "encoded = ks[cat_features].apply(encoder.fit_transform)\n",
    "\n",
    "data_cols = ['goal', 'hour', 'day', 'month', 'year', 'outcome']\n",
    "baseline_data = ks[data_cols].join(encoded)\n",
    "\n",
    "cat_features = ['category', 'currency', 'country']\n",
    "interactions = pd.DataFrame(index=ks.index)\n",
    "for col1, col2 in itertools.combinations(cat_features, 2):\n",
    "    new_col_name = '_'.join([col1, col2])\n",
    "    # Convert to strings and combine\n",
    "    new_values = ks[col1].map(str) + \"_\" + ks[col2].map(str)\n",
    "    label_enc = LabelEncoder()\n",
    "    interactions[new_col_name] = label_enc.fit_transform(new_values)\n",
    "baseline_data = baseline_data.join(interactions)\n",
    "\n",
    "launched = pd.Series(ks.index, index=ks.launched, name=\"count_7_days\").sort_index()\n",
    "count_7_days = launched.rolling('7d').count() - 1\n",
    "count_7_days.index = launched.values\n",
    "count_7_days = count_7_days.reindex(ks.index)\n",
    "\n",
    "baseline_data = baseline_data.join(count_7_days)\n",
    "\n",
    "def time_since_last_project(series):\n",
    "    # Return the time in hours\n",
    "    return series.diff().dt.total_seconds() / 3600.\n",
    "\n",
    "df = ks[['category', 'launched']].sort_values('launched')\n",
    "timedeltas = df.groupby('category').transform(time_since_last_project)\n",
    "timedeltas = timedeltas.fillna(timedeltas.max())\n",
    "\n",
    "baseline_data = baseline_data.join(timedeltas.rename({'launched': 'time_since_last_project'}, axis=1))\n",
    "\n",
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "    valid_fraction = 0.1\n",
    "    valid_size = int(len(dataframe) * valid_fraction)\n",
    "\n",
    "    train = dataframe[:-valid_size * 2]\n",
    "    # valid size == test size, last two sections of the data\n",
    "    valid = dataframe[-valid_size * 2:-valid_size]\n",
    "    test = dataframe[-valid_size:]\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "def train_model(train, valid):\n",
    "    feature_cols = train.columns.drop('outcome')\n",
    "\n",
    "    dtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\n",
    "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n",
    "\n",
    "    param = {'num_leaves': 64, 'objective': 'binary', \n",
    "             'metric': 'auc', 'seed': 7}\n",
    "    print(\"Training model!\")\n",
    "    bst = lgb.train(param, dtrain, num_boost_round=1000, valid_sets=[dvalid], \n",
    "                    early_stopping_rounds=10, verbose_eval=False)\n",
    "\n",
    "    valid_pred = bst.predict(valid[feature_cols])\n",
    "    valid_score = metrics.roc_auc_score(valid['outcome'], valid_pred)\n",
    "    print(f\"Validation AUC score: {valid_score:.4f}\")\n",
    "    return bst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
